import React, { useState, useCallback, useEffect } from 'react';
import { FileUploader } from './components/FileUploader';
import { VideoPlayer } from './components/VideoPlayer';
import { TranscriptionPane } from './components/TranscriptionPane';
import { ClipConfigurationPane } from './components/ClipConfigurationPane';
import { CaptionControls } from './components/CaptionControls';
import { SuggestedSegmentItem } from './components/SuggestedSegmentItem';
import { analyzeTranscriptForSegments } from './services/geminiService';
import { AppStep } from './types';
import type { Segment, Clip, Caption } from './types';
import { InfoIcon, LightBulbIcon, ScissorsIcon, TextAaIcon, PlayIcon, UploadIcon, WandSparklesIcon, WarningIcon, RocketLaunchIcon } from './components/icons';

const App: React.FC = () => {
  const [videoFile, setVideoFile] = useState<File | null>(null);
  const [videoSrc, setVideoSrc] = useState<string | null>(null);
  const [transcription, setTranscription] = useState<string>(''); // Will store simulated transcript
  const [isSimulatingLocalTranscription, setIsSimulatingLocalTranscription] = useState<boolean>(false);
  const [isLoadingGeminiAnalysis, setIsLoadingGeminiAnalysis] = useState<boolean>(false);
  const [suggestedSegments, setSuggestedSegments] = useState<Segment[]>([]);
  const [selectedSegmentForClipping, setSelectedSegmentForClipping] = useState<Segment | null>(null);
  const [currentClipConfig, setCurrentClipConfig] = useState<Clip | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [apiKeyError, setApiKeyError] = useState<string | null>(null);
  const [currentStep, setCurrentStep] = useState<AppStep>(AppStep.Upload);

  useEffect(() => {
    if (!process.env.API_KEY) {
      setApiKeyError("API_KEY environment variable not set. Gemini AI-powered segment analysis will not work.");
      console.warn("API_KEY environment variable not set. Gemini AI-powered segment analysis will not work.");
    }
  }, []);

  const handleFileSelect = useCallback((file: File) => {
    setVideoFile(file);
    const url = URL.createObjectURL(file);
    setVideoSrc(url);
    setTranscription('');
    setSuggestedSegments([]);
    setSelectedSegmentForClipping(null);
    setCurrentClipConfig(null);
    setError(null);
    setCurrentStep(AppStep.GenerateTranscript);
  }, []);

  const handleStartLocalProcessingChain = useCallback(async () => {
    if (!process.env.API_KEY) {
        setError("Cannot analyze transcript: API_KEY environment variable not set for Gemini.");
        setApiKeyError("API_KEY environment variable not set for Gemini."); // Ensure this is visible
        return;
    }
    setIsSimulatingLocalTranscription(true);
    setError(null);
    setApiKeyError(null); // Clear previous API key error if we attempt to run
    setSuggestedSegments([]);
    setTranscription(''); // Clear previous simulated transcript

    // Simulate local transcription delay
    await new Promise(resolve => setTimeout(resolve, 2000)); 
    const dummyTranscript = `This is a simulated transcript generated by a conceptual local AI (e.g., Whisper running on your machine). Key moments in a video might include: an exciting product reveal ("Today, we're unveiling something truly groundbreaking!"), a detailed explanation of a complex feature ("Let's dive into how the quantum entanglement module functions..."), or a surprising event capture ("Wow, did you see that meteor shower just now?!"). This text is then passed to Gemini AI to find highlight-worthy segments. For example, a user might talk about their travel vlog: "Our first stop was the Eiffel Tower, it was breathtaking. Then we tried some amazing croissants. Later, we got lost in the Louvre, what an adventure!"`;
    setTranscription(dummyTranscript);
    setIsSimulatingLocalTranscription(false);

    // Now, proceed with Gemini analysis using the simulated transcript
    setIsLoadingGeminiAnalysis(true);
    try {
      const segments = await analyzeTranscriptForSegments(dummyTranscript);
      setSuggestedSegments(segments);
      setCurrentStep(AppStep.SelectSegment);
      if (segments.length === 0) {
        setError("Gemini AI could not identify distinct segments from the simulated transcript. Try again, or imagine a different transcript for better results.");
      }
    } catch (e) {
      console.error("Error analyzing transcript with Gemini:", e);
      setError(`Failed to analyze transcript with Gemini. ${e instanceof Error ? e.message : 'Unknown error'}`);
    } finally {
      setIsLoadingGeminiAnalysis(false);
    }
  }, []);


  const handleSelectSegment = useCallback((segment: Segment) => {
    setSelectedSegmentForClipping(segment);
    const newClip: Clip = {
      id: segment.id,
      title: segment.title,
      startCue: segment.startCue,
      endCue: segment.endCue,
      startTimeSeconds: 0, 
      endTimeSeconds: 15, 
      autoFrame: true,
      aspectRatio: '9:16', 
      captions: [],
    };
    setCurrentClipConfig(newClip);
    setCurrentStep(AppStep.ConfigureClip);
  }, []);

  const updateClipConfig = useCallback((configUpdater: (prevConfig: Clip | null) => Clip | null) => {
    setCurrentClipConfig(prevConfig => configUpdater(prevConfig));
  }, []);

  const addCaptionToClip = useCallback((text: string) => {
    if (!currentClipConfig) return;
    const newCaption: Caption = {
      id: `caption-${Date.now()}`,
      text,
      startTime: 0, 
      endTime: 5,   
      style: {
        font: 'Arial',
        size: 24,
        color: '#FFFFFF',
        position: 'bottom',
        backgroundColor: 'rgba(0,0,0,0.5)',
      },
    };
    updateClipConfig(prev => prev ? ({ ...prev, captions: [...prev.captions, newCaption] }) : null);
  }, [currentClipConfig, updateClipConfig]);

  const updateCaptionInClip = useCallback((captionId: string, updatedCaption: Caption) => {
    updateClipConfig(prev => {
      if (!prev) return null;
      return {
        ...prev,
        captions: prev.captions.map(c => c.id === captionId ? updatedCaption : c)
      };
    });
  }, [updateClipConfig]);

  const removeCaptionFromClip = useCallback((captionId: string) => {
    updateClipConfig(prev => {
      if (!prev) return null;
      return {
        ...prev,
        captions: prev.captions.filter(c => c.id !== captionId)
      };
    });
  }, [updateClipConfig]);

  const CurrentStepIcon = ({step}: {step: AppStep}) => {
    switch(step) {
      case AppStep.Upload: return <UploadIcon className="w-6 h-6 mr-2" />;
      case AppStep.GenerateTranscript: return <RocketLaunchIcon className="w-6 h-6 mr-2" />;
      case AppStep.SelectSegment: return <LightBulbIcon className="w-6 h-6 mr-2" />;
      case AppStep.ConfigureClip: return <ScissorsIcon className="w-6 h-6 mr-2" />;
      default: return <PlayIcon className="w-6 h-6 mr-2" />;
    }
  };
  
  const getStepName = (step: AppStep): string => {
    switch(step) {
      case AppStep.Upload: return "Upload Video";
      case AppStep.GenerateTranscript: return "Generate Transcript & Analyze Segments (Simulated)";
      case AppStep.SelectSegment: return "Select AI Suggested Segment";
      case AppStep.ConfigureClip: return "Configure Your Clip";
      default: return "Process";
    }
  };

  return (
    <div className="min-h-screen flex flex-col items-center p-4 md:p-8 bg-gradient-to-br from-gray-900 to-slate-800">
      <header className="w-full max-w-5xl mb-8 text-center">
        <h1 className="text-4xl md:text-5xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-purple-400 via-pink-500 to-red-500">
          Local Video Clipper AI
        </h1>
        <p className="text-gray-400 mt-2 text-sm md:text-base">
          AI-assisted workflow for identifying key video moments using local processing concepts & Gemini API.
        </p>
      </header>

      {apiKeyError && currentStep !== AppStep.GenerateTranscript && ( // Show API key error unless we are on the step that might fix it by running
         <div className="w-full max-w-3xl bg-red-800 border border-red-700 text-white px-4 py-3 rounded-lg relative mb-4" role="alert">
          <strong className="font-bold">Configuration Error:</strong>
          <span className="block sm:inline ml-2">{apiKeyError}</span>
        </div>
      )}


      <main className="w-full max-w-5xl space-y-8">
        <div className="bg-gray-800 shadow-2xl rounded-xl p-6 md:p-8">
          <div className="flex items-center text-2xl font-semibold text-purple-300 mb-6 pb-2 border-b-2 border-purple-500">
            <CurrentStepIcon step={currentStep} />
            Workflow Step: {getStepName(currentStep)}
          </div>

          {currentStep === AppStep.Upload && (
            <FileUploader onFileSelect={handleFileSelect} />
          )}

          {videoFile && (currentStep === AppStep.GenerateTranscript || currentStep === AppStep.SelectSegment || currentStep === AppStep.ConfigureClip) && (
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
              <VideoPlayer src={videoSrc} fileName={videoFile.name} />
              <div className="bg-gray-700 p-4 rounded-lg shadow-inner">
                <h3 className="text-lg font-semibold text-pink-400 mb-2">Video Details</h3>
                <p className="text-sm text-gray-300"><strong>Name:</strong> {videoFile.name}</p>
                <p className="text-sm text-gray-300"><strong>Type:</strong> {videoFile.type || 'N/A'}</p>
                <p className="text-sm text-gray-300"><strong>Size:</strong> {(videoFile.size / (1024*1024)).toFixed(2)} MB</p>
                <div className="mt-4 p-3 bg-yellow-900 bg-opacity-30 border border-yellow-700 rounded-md text-xs text-yellow-300">
                  <WarningIcon className="w-4 h-4 inline mr-1" />
                  This app simulates a local AI workflow. Actual transcription (e.g., with Whisper) and video processing (e.g., with FFmpeg, using GPU for auto-framing/rendering) must be done with your own local tools.
                </div>
              </div>
            </div>
          )}

          {currentStep === AppStep.GenerateTranscript && videoFile && (
            <TranscriptionPane
              onStartProcessing={handleStartLocalProcessingChain}
              isSimulatingLocalTranscription={isSimulatingLocalTranscription}
              isAnalyzingWithGemini={isLoadingGeminiAnalysis}
              simulatedTranscript={transcription}
              apiKeyMissing={!process.env.API_KEY}
            />
          )}
          
          {currentStep === AppStep.SelectSegment && transcription && !isLoadingGeminiAnalysis && !isSimulatingLocalTranscription && (
            suggestedSegments.length > 0 ? (
              <div className="space-y-4">
                <h3 className="text-xl font-semibold text-pink-400 flex items-center"><LightBulbIcon className="w-6 h-6 mr-2"/>AI Suggested Segments (from Simulated Transcript)</h3>
                <p className="text-gray-400 text-sm">Select a segment to configure as a clip. These are based on Gemini AI's analysis of the simulated locally-generated transcript.</p>
                {suggestedSegments.map((segment) => (
                  <SuggestedSegmentItem key={segment.id} segment={segment} onSelect={handleSelectSegment} />
                ))}
              </div>
            ) : (
              <div className="text-center py-4 text-gray-400">
                <InfoIcon className="w-8 h-8 mx-auto mb-2 text-blue-400"/>
                <p>No segments were suggested by Gemini AI from the simulated transcript.</p>
                <p>You can try running the "Generate Transcript & Analyze Segments" step again, or if this were a real local setup, you'd check your local transcription quality.</p>
                <button 
                    onClick={() => setCurrentStep(AppStep.GenerateTranscript)}
                    className="mt-4 bg-purple-600 hover:bg-purple-700 text-white font-semibold py-2 px-4 rounded-lg shadow-md transition duration-150 ease-in-out"
                  >
                    Back to Transcript Generation
                </button>
              </div>
            )
          )}


          {currentStep === AppStep.ConfigureClip && currentClipConfig && (
            <div className="space-y-6">
              <div>
                <h3 className="text-xl font-semibold text-pink-400 flex items-center"><ScissorsIcon className="w-6 h-6 mr-2"/>Configure Clip: <span className="text-purple-300 ml-2">{currentClipConfig.title}</span></h3>
                 <p className="text-sm text-gray-400 mb-4">Adjust timing, aspect ratio, and add captions. Remember, auto-framing and final rendering would use your local GPU-accelerated tools with these settings.</p>
                <ClipConfigurationPane
                  clipConfig={currentClipConfig}
                  updateClipConfig={updateClipConfig}
                />
              </div>
              <div>
                 <h3 className="text-xl font-semibold text-pink-400 flex items-center mb-2"><TextAaIcon className="w-6 h-6 mr-2"/>Captions</h3>
                <CaptionControls
                  captions={currentClipConfig.captions}
                  onAddCaption={addCaptionToClip}
                  onUpdateCaption={updateCaptionInClip}
                  onRemoveCaption={removeCaptionFromClip}
                  clipDuration={currentClipConfig.endTimeSeconds - currentClipConfig.startTimeSeconds}
                />
              </div>
              <div className="mt-6 p-4 bg-gray-700 rounded-lg">
                <h4 className="text-lg font-semibold text-green-400 mb-2 flex items-center"><WandSparklesIcon className="w-5 h-5 mr-2" />Export Configuration for Local Tools</h4>
                <p className="text-sm text-gray-300 mb-3">
                  Use this JSON configuration with your local video editing scripts/tools (e.g., FFmpeg). It includes segment timings, aspect ratio, auto-frame preference, and caption details.
                </p>
                <pre className="text-xs bg-gray-800 p-3 rounded-md overflow-x-auto text-green-300">
                  {JSON.stringify(currentClipConfig, null, 2)}
                </pre>
                 <button 
                    onClick={() => setCurrentStep(AppStep.SelectSegment)}
                    className="mt-4 bg-blue-600 hover:bg-blue-700 text-white font-semibold py-2 px-4 rounded-lg shadow-md transition duration-150 ease-in-out mr-2"
                  >
                    Back to Segments
                  </button>
                  <button 
                    onClick={() => {
                      setVideoFile(null);
                      setVideoSrc(null);
                      setTranscription('');
                      setSuggestedSegments([]);
                      setSelectedSegmentForClipping(null);
                      setCurrentClipConfig(null);
                      setError(null);
                      setCurrentStep(AppStep.Upload);
                    }}
                    className="mt-4 bg-red-600 hover:bg-red-700 text-white font-semibold py-2 px-4 rounded-lg shadow-md transition duration-150 ease-in-out"
                  >
                    Start Over with New Video
                  </button>
              </div>
            </div>
          )}
          
          {error && (
            <div className="mt-4 bg-red-800 border border-red-600 text-white px-4 py-3 rounded-lg relative" role="alert">
              <strong className="font-bold">Error:</strong>
              <span className="block sm:inline ml-2">{error}</span>
              <button onClick={() => setError(null)} className="absolute top-0 bottom-0 right-0 px-4 py-3 text-red-200 hover:text-white">
                <span className="text-2xl">&times;</span>
              </button>
            </div>
          )}
        </div>
      </main>

      <footer className="w-full max-w-5xl mt-12 text-center text-gray-500 text-xs">
        <p>&copy; {new Date().getFullYear()} Local Video Clipper AI. For demonstration and conceptual purposes.</p>
        <p>This tool simulates a workflow involving local AI (like Whisper for transcription, FFmpeg for processing) and uses Gemini API for transcript analysis. Actual local processing is not performed by this web UI.</p>
      </footer>
    </div>
  );
};

export default App;